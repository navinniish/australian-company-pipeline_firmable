# Australian Company Data Pipeline

A comprehensive ETL pipeline that extracts, transforms, and loads Australian company data from Common Crawl and the Australian Business Register (ABR) into a PostgreSQL database, with intelligent entity matching using Large Language Models.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Common Crawl  â”‚    â”‚       ABR       â”‚    â”‚   PostgreSQL     â”‚
â”‚   (~200k URLs) â”‚    â”‚  (XML Extracts) â”‚    â”‚    Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETL PIPELINE                                 â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Extractors  â”‚  â”‚ Transformersâ”‚  â”‚   Loaders   â”‚            â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚            â”‚
â”‚  â”‚ â€¢ CC Extractâ”‚  â”‚ â€¢ LLM Match â”‚  â”‚ â€¢ Core DB   â”‚            â”‚
â”‚  â”‚ â€¢ ABR Parse â”‚  â”‚ â€¢ Clean     â”‚  â”‚ â€¢ Analytics â”‚            â”‚
â”‚  â”‚ â€¢ Async     â”‚  â”‚ â€¢ Normalize â”‚  â”‚ â€¢ dbt Modelsâ”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              LLM Entity Matching                        â”‚   â”‚
â”‚  â”‚  â€¢ OpenAI GPT-4 / Anthropic                           â”‚   â”‚
â”‚  â”‚  â€¢ Semantic Similarity + Rule-based                    â”‚   â”‚
â”‚  â”‚  â€¢ Smart Prompts for Complex Cases                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STORAGE LAYER                                â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚   Staging    â”‚ â”‚     Core     â”‚ â”‚       Analytics          â”‚ â”‚
â”‚ â”‚              â”‚ â”‚              â”‚ â”‚                          â”‚ â”‚
â”‚ â”‚ â€¢ raw_cc     â”‚ â”‚ â€¢ companies  â”‚ â”‚ â€¢ summary_stats          â”‚ â”‚
â”‚ â”‚ â€¢ raw_abr    â”‚ â”‚ â€¢ contacts   â”‚ â”‚ â€¢ industry_dist          â”‚ â”‚
â”‚ â”‚ â€¢ matches    â”‚ â”‚ â€¢ names      â”‚ â”‚ â€¢ geographic_analysis    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Key Features

### Data Sources Integration
- **Common Crawl**: Extracts ~200,000 Australian company websites (.au domains)
- **Australian Business Register**: Processes bulk XML exports with company registrations
- **Smart Filtering**: Identifies likely company pages vs blogs/personal sites

### Advanced Entity Matching
- **LLM-Powered Matching**: Uses OpenAI GPT-4 or Anthropic for intelligent company matching
- **Multi-Layer Approach**: Combines rule-based filtering, semantic similarity, and LLM reasoning
- **Confidence Scoring**: Provides confidence levels and reasoning for each match
- **Manual Review Flags**: Identifies matches requiring human verification

### Data Quality & Validation
- **ABN Validation**: Full checksum validation for Australian Business Numbers
- **Data Quality Scores**: Comprehensive scoring based on completeness and reliability
- **dbt Testing**: Automated data quality tests and validation
- **Duplicate Detection**: Identifies potential duplicates across sources

### Scalable Architecture
- **Async Processing**: Handles large volumes (100k+ records) efficiently
- **Batch Processing**: Configurable batch sizes for memory management
- **Database Optimization**: Proper indexing and query optimization
- **Docker Support**: Full containerization for easy deployment

## ğŸ“Š Database Schema

### Core Tables Structure

```sql
-- Main companies table (unified view)
core.companies
â”œâ”€â”€ company_id (UUID, PK)
â”œâ”€â”€ abn (VARCHAR(11), unique)
â”œâ”€â”€ company_name (TEXT)
â”œâ”€â”€ normalized_name (TEXT, indexed)
â”œâ”€â”€ website_url (TEXT)
â”œâ”€â”€ industry (TEXT)
â”œâ”€â”€ address details (multiple columns)
â”œâ”€â”€ data_quality_score (DECIMAL 0-1)
â””â”€â”€ audit fields

-- Alternative names
core.company_names
â”œâ”€â”€ company_id (FK)
â”œâ”€â”€ name (TEXT)
â”œâ”€â”€ name_type (trading|business|legal)
â””â”€â”€ is_primary (BOOLEAN)

-- Contact information
core.company_contacts
â”œâ”€â”€ company_id (FK)
â”œâ”€â”€ contact_type (email|phone|social)
â”œâ”€â”€ contact_value (TEXT)
â””â”€â”€ is_verified (BOOLEAN)

-- Industry classifications
core.industry_classifications
â”œâ”€â”€ company_id (FK)
â”œâ”€â”€ classification_system (VARCHAR)
â”œâ”€â”€ code (VARCHAR)
â””â”€â”€ confidence_score (DECIMAL)
```

### Staging & Analytics

```sql
-- Staging tables for raw data
staging.common_crawl_raw
staging.abr_raw
staging.entity_matching_candidates

-- Analytics tables
analytics.companies_by_state
analytics.industry_distribution
analytics.data_quality_summary
```

## ğŸ”§ Technology Stack

| Component | Technology | Rationale |
|-----------|------------|-----------|
| **Database** | PostgreSQL 15 | ACID compliance, JSON support, full-text search, scalability |
| **Pipeline** | Python + asyncio | High performance async processing for I/O bound tasks |
| **Entity Matching** | OpenAI GPT-4 / Anthropic | State-of-the-art reasoning for complex matching scenarios |
| **Data Transformation** | dbt | SQL-based transformations, testing, documentation |
| **Orchestration** | Built-in Python | Lightweight, customizable, good for MVP |
| **Containerization** | Docker + Docker Compose | Easy deployment, environment consistency |
| **Data Quality** | Great Expectations + dbt tests | Comprehensive validation and monitoring |

### Alternative Technologies Considered

- **Spark**: Overkill for current data volumes, adds complexity
- **Airflow**: Heavy orchestration, Python-based solution is simpler for this use case
- **Kafka**: Not needed for batch processing, would be useful for real-time streaming
- **ClickHouse**: PostgreSQL adequate for current analytics needs

## ğŸ¤– AI Model Usage & Rationale

### Primary LLM: OpenAI GPT-4 Turbo
- **Why GPT-4**: Superior reasoning capabilities for entity matching
- **Temperature 0.3**: Balance between consistency and flexibility  
- **Max Tokens 2000**: Sufficient for detailed reasoning explanations

### LLM Integration Points

#### 1. Company Information Extraction
```python
# Extract company data from web pages
prompt = f"""
You are analyzing an Australian company website to extract key business information.

Website URL: {url}
Page Title: {title}
Meta Description: {description}
Page Content: {content}

Please extract the following information and return as JSON:
{{
    "company_name": "Official company name (string or null)",
    "industry": "Primary industry/business sector (string or null)", 
    "contact_info": {{
        "email": "Contact email if found",
        "phone": "Phone number if found",
        "address": "Physical address if found"
    }},
    "confidence": "Confidence score 0.0-1.0 for extraction quality"
}}
"""
```

#### 2. Entity Matching Verification
```python
# Verify if two company records represent the same entity
prompt = f"""
You are an expert in entity matching for Australian business data.

COMMON CRAWL RECORD:
- Website URL: {cc_record['website_url']}
- Company Name: {cc_record['company_name']}
- Industry: {cc_record['industry']}

ABR RECORD:
- ABN: {abr_record['abn']}
- Entity Name: {abr_record['entity_name']}
- Trading Names: {abr_record['trading_names']}
- Location: {abr_record['address']}

Please analyze and return your response as JSON:
{{
    "is_match": true/false,
    "confidence": 0.0-1.0,
    "reasoning": "Detailed explanation of your decision",
    "key_factors": ["list", "of", "key", "matching", "factors"]
}}

Consider name variations, domain alignment, and Australian business conventions.
"""
```

#### 3. Industry Classification
```python
# Classify business into standard industry categories
prompt = f"""
Classify this business into one of these Australian industry categories:
- Manufacturing
- Construction  
- Professional Services
- Technology
- Retail Trade
[...]

Business information: {business_text}

Return only the category name.
"""
```

### Cost Optimization
- **Batch Processing**: Process multiple records together when possible
- **Caching**: Cache LLM responses for similar queries
- **Tiered Approach**: Use rules-based filtering before LLM verification
- **Cost Monitoring**: Track token usage and estimated costs

## ğŸƒâ€â™‚ï¸ Getting Started

### Prerequisites
- Python 3.11+
- PostgreSQL 15+
- Docker & Docker Compose
- OpenAI API Key or Anthropic API Key

### Quick Start with Docker

1. **Clone and configure**:
```bash
git clone <repository-url>
cd australian-company-pipeline
cp .env.example .env
# Edit .env with your API keys and database credentials
```

2. **Start services**:
```bash
docker-compose -f docker/docker-compose.yml up -d
```

3. **Initialize database**:
```bash
docker-compose exec postgres psql -U postgres -d australian_companies -f /docker-entrypoint-initdb.d/01_create_schemas.sql
```

4. **Run the pipeline**:
```bash
docker-compose exec etl_pipeline python -m src.pipeline.etl_pipeline
```

### Manual Installation

1. **Install dependencies**:
```bash
pip install -r requirements.txt
```

2. **Set up database**:
```bash
createdb australian_companies
psql australian_companies -f sql/ddl/01_create_schemas.sql
psql australian_companies -f sql/ddl/02_staging_tables.sql
# ... run all DDL files in order
```

3. **Configure environment**:
```bash
export DATABASE_URL="postgresql://user:pass@localhost:5432/australian_companies"
export OPENAI_API_KEY="your-api-key-here"
```

4. **Run pipeline**:
```bash
python -m src.pipeline.etl_pipeline
```

### Running dbt Models

```bash
cd dbt/
dbt deps
dbt run
dbt test
```

## ğŸ“‹ Configuration

### Environment Variables

```bash
# Database Configuration
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=australian_companies
DATABASE_USER=postgres
DATABASE_PASSWORD=your_password

# LLM Configuration  
LLM_PROVIDER=openai  # or 'anthropic'
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

# Pipeline Configuration
CC_MAX_RECORDS=200000
ABR_MAX_RECORDS=1000000
EXTRACT_BATCH_SIZE=1000
CONCURRENT_REQUESTS=10

# Entity Matching Thresholds
EXACT_MATCH_THRESHOLD=0.95
HIGH_CONFIDENCE_THRESHOLD=0.85
LLM_REVIEW_THRESHOLD=0.60
MANUAL_REVIEW_THRESHOLD=0.40
```

### Pipeline Modes

#### Full Pipeline
```bash
python -m src.pipeline.etl_pipeline
```

#### Incremental Update
```bash
python -m src.pipeline.etl_pipeline --incremental
```

#### Status Check
```bash
python -m src.pipeline.etl_pipeline --status
```

## ğŸ§ª Testing & Data Quality

### dbt Tests
```bash
dbt test  # Run all data quality tests
```

### Manual Quality Checks
```bash
# ABN validation
SELECT COUNT(*) FROM core.companies WHERE abn IS NOT NULL AND length(abn) != 11;

# Duplicate detection
SELECT normalized_name, COUNT(*) FROM core.companies GROUP BY normalized_name HAVING COUNT(*) > 1;

# Data quality distribution
SELECT quality_tier, COUNT(*) FROM staging_dbt.stg_companies GROUP BY quality_tier;
```

### Test Coverage
- âœ… ABN format and checksum validation
- âœ… Data quality score ranges
- âœ… Duplicate company detection
- âœ… Address format validation
- âœ… Contact information validation
- âœ… Entity matching confidence thresholds

## ğŸ“ˆ Performance & Scaling

### Current Capacity
- **Common Crawl**: 200,000 websites processed in ~4 hours
- **ABR Processing**: 1M+ records processed in ~2 hours  
- **Entity Matching**: ~50,000 matches per hour with LLM verification
- **Database**: Optimized for 10M+ company records

### Optimization Techniques
- **Async Processing**: Concurrent I/O operations
- **Database Indexing**: Strategic indexes on search columns
- **Batch Operations**: Bulk inserts and updates
- **Connection Pooling**: Efficient database connections
- **LLM Caching**: Avoid duplicate API calls

### Scaling Recommendations
- **Horizontal Scaling**: Add more worker containers
- **Database Sharding**: Partition by state/industry for very large datasets
- **CDN Caching**: Cache static API responses
- **Queue System**: Add Redis/RabbitMQ for job queuing

## ğŸ”’ Security & Compliance

### Data Security
- **API Key Management**: Environment variables, no hardcoding
- **Database Security**: Role-based access control
- **Data Masking**: PII protection in logs
- **Audit Logging**: Full pipeline execution tracking

### Access Control
```sql
-- Read-only analyst access
GRANT SELECT ON ALL TABLES IN SCHEMA core TO pipeline_analyst;

-- ETL service access
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA staging TO pipeline_etl;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA core TO pipeline_etl;
```

### Compliance Considerations
- **Data Retention**: Configurable cleanup of staging data
- **Privacy**: No personal data collection beyond business info
- **Attribution**: Proper attribution to data sources
- **Rate Limiting**: Respectful API usage

## ğŸ› Troubleshooting

### Common Issues

#### Database Connection Errors
```bash
# Check PostgreSQL is running
docker-compose ps postgres

# Test connection
psql "postgresql://postgres:password@localhost:5432/australian_companies" -c "SELECT 1;"
```

#### LLM API Failures
```bash
# Check API key
python -c "import openai; print('API key configured' if openai.api_key else 'No API key')"

# Test API connection
curl -H "Authorization: Bearer $OPENAI_API_KEY" https://api.openai.com/v1/models
```

#### Memory Issues
```bash
# Reduce batch size
export EXTRACT_BATCH_SIZE=100
export MATCHING_BATCH_SIZE=50
```

### Performance Issues

#### Slow Entity Matching
- Reduce `LLM_REVIEW_THRESHOLD` to use fewer LLM calls
- Increase `MANUAL_REVIEW_THRESHOLD` to skip low-confidence matches
- Scale horizontally with more workers

#### Database Performance
```sql
-- Monitor query performance
SELECT query, mean_exec_time, calls FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10;

-- Check index usage
SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read FROM pg_stat_user_indexes WHERE idx_scan = 0;
```

## ğŸ“š Development

### IDE Setup
**Primary IDE**: Visual Studio Code with Python extension pack

**Recommended Extensions**:
- Python
- PostgreSQL (by Chris Kolkman) 
- Docker
- GitLens
- dbt Power User

### Code Structure
```
src/
â”œâ”€â”€ extractors/          # Data extraction modules
â”œâ”€â”€ transformers/        # Data cleaning and transformation
â”œâ”€â”€ loaders/            # Database loading utilities
â”œâ”€â”€ entity_matching/    # LLM-based matching logic
â”œâ”€â”€ pipeline/           # Main orchestration
â””â”€â”€ utils/              # Shared utilities

dbt/
â”œâ”€â”€ models/             # dbt transformation models
â”œâ”€â”€ tests/              # Data quality tests
â””â”€â”€ macros/             # Reusable SQL macros
```

### Contributing Guidelines
1. Follow PEP 8 for Python code
2. Add type hints to all functions
3. Write comprehensive docstrings
4. Add appropriate tests for new features
5. Update documentation for major changes

### Testing
```bash
# Run Python tests
pytest tests/

# Run dbt tests
cd dbt && dbt test

# Run integration tests
pytest tests/integration/
```

## ğŸ“Š Monitoring & Metrics

### Key Metrics
- **Extraction Success Rate**: % of URLs successfully processed
- **Entity Matching Accuracy**: Manual verification of sample matches
- **Data Quality Score Distribution**: Monitor data quality trends
- **Processing Time**: Track pipeline execution duration
- **API Cost**: Monitor LLM API usage and costs

### Monitoring Setup
```bash
# Prometheus metrics endpoint
curl http://localhost:8000/metrics

# Database performance
SELECT * FROM pg_stat_activity WHERE state = 'active';

# Pipeline logs
docker-compose logs -f etl_pipeline
```

## ğŸš€ Deployment

### Production Checklist
- [ ] Environment variables configured
- [ ] Database backup strategy in place
- [ ] Monitoring and alerting configured  
- [ ] API rate limits configured
- [ ] Data retention policies set
- [ ] Security hardening completed
- [ ] Load testing performed

### CI/CD Pipeline
```yaml
# .github/workflows/deploy.yml
name: Deploy Pipeline
on:
  push:
    branches: [main]
    
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: pytest
        
  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to production
        run: docker-compose up -d --build
```

## ğŸ“œ License & Attribution

### Data Sources
- **Common Crawl**: [Common Crawl Terms of Use](https://commoncrawl.org/terms-of-use/)
- **Australian Business Register**: [data.gov.au License](https://data.gov.au/dataset/5bd7fcab-e315-42cb-8dcc-22b8f8460c07)

### Third-Party Dependencies
See `requirements.txt` for full list of dependencies and their licenses.

---

## ğŸ”— API Reference

### Pipeline Status API
```bash
# Get current pipeline status
GET /api/status

# Get recent pipeline runs
GET /api/runs?limit=10

# Trigger manual pipeline run
POST /api/run {"incremental": false}
```

### Data Access API
```bash
# Search companies
GET /api/companies?search=acme&state=NSW&limit=50

# Get company details
GET /api/companies/{company_id}

# Get data quality metrics
GET /api/metrics/quality
```

#   a u s t r a l i a n - c o m p a n y - p i p e l i n e _ p r o j e c t 
 
 #   a u s t r a l i a n - c o m p a n y - p i p e l i n e _ p r o j e c t _ f i r m a b l e 
 
 #   a u s t r a l i a n - c o m p a n y - p i p e l i n e _ f i r m a b l e 
 
 
